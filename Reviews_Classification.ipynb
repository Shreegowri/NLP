{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reviews_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc2R3x6QSIFi",
        "colab_type": "text"
      },
      "source": [
        "In this experiment, you will explore the accuracy of sentiment classificaiton using different feature representations of text documents.\n",
        "\n",
        "First, you will implement `createBasicFeatures`, which creates a sparse matrix representation of a collection of documents. For this exercise, you should have a feature for each word containing at least one alphabetic character. You may use the `numpy` and `sklearn` packages to help with implementing a sparse matrix.\n",
        "\n",
        "Then, you will implement `createFancyFeatures`, which can specify at any other features you choose to help improve performance on the classification task.\n",
        "\n",
        "The two code blocks at the end train and evaluate two models—logistic regression with L1 and L2 regularization—using your featurization functions. Besides held-out classification accuracy with 10-fold cross-validation, you will also see the features in each class given high weights by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6kK4LZlLKzz",
        "colab_type": "text"
      },
      "source": [
        "# **Observartions**\n",
        "\n",
        "*Features of pos class have postive words such as fun, great, excellent and so on while feature such as bad, worst, awful corresponds correctly to neg class. We also see words which are related to plot of the movie in neg class.*\n",
        "*Achieved accuracy of 84.2%*\n",
        "\n",
        "\n",
        "1. TF-IDF representation did not work well than word count representation \n",
        "2. Bi-grams did not perform as good as uni-grams for this corpus. \n",
        "3. Noun parts of speech for lemmatization works better than verb and adjective\n",
        "4. Allowing words which are present at most on 80% of the reviews and appeared in atleast 3 reviews\n",
        "5. Included stopwords which are part of CountVectorizer function not the stopwords from nltk package as these have negation (not, shouldn't, ...) words which could be usual for differentiating neg class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVS67_HNRmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate,LeaveOneOut,KFold\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzjMY8fYQbB6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# read in the movie review corpus\n",
        "def readReviews():\n",
        "  raw = requests.get(\"https://raw.githubusercontent.com/mutherr/CS6120-PS1-data/master/cornell_reviews.json\").text.strip()\n",
        "  corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvd3I95FT01D",
        "colab_type": "text"
      },
      "source": [
        "This is where you will implement two functions to featurize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYkl0whj0e90",
        "colab_type": "code",
        "outputId": "e2600afb-3972-42fe-ffad-30e2500feb90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer #for lemmatization\n",
        "import re #regular expression package\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039fPQcF7OkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NB: The current contents are for testing only\n",
        "#This function should return: \n",
        "#  -a sparse numpy matrix of document features\n",
        "#  -a list of the correct class for each document\n",
        "#  -a list of the vocabulary used by the features, such that the ith term of the\n",
        "#    list is the word whose counts appear in the ith column of the matrix. \n",
        "\n",
        "# This function should create a feature representation using all tokens that\n",
        "# contain an alphabetic character.\n",
        "def createBasicFeatures(corpus):\n",
        "  X = [dict['text'] for dict in corpus] #list of reviews Or map(lambda dict: dict['text'], corpus)\n",
        "  classes = [ dict['class'] for dict in corpus] #list of correct classes\n",
        "\n",
        "  #Pre-process the data\n",
        "  docs = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  for rev in range(0, len(X)):\n",
        "    #remove all the special characters\n",
        "    doc = re.sub(r'[\\W_]', ' ', str(X[rev])) #introduces multiple spaces\n",
        "    #\\W ==> string does not contain any word characters\n",
        "\n",
        "    #remove all single characters (previous step would have introduced lot of 's')\n",
        "    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
        "    doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
        "\n",
        "    #substitute multiple spaces with single space\n",
        "    doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n",
        "\n",
        "    #remove prefix 'b' (if the dataset is in bytes, each line will have letter 'b'appended at the start)\n",
        "    doc = re.sub(r'^b\\s+', '', doc)\n",
        "\n",
        "    #there are lot of numbers, remove the numbers as they don't add predictive power\n",
        "    doc = re.sub(r'[0-9]+', '', doc) \n",
        "\n",
        "    doc = doc.lower() # converting to lowercase\n",
        "    docs.append(doc)\n",
        "  \n",
        "  vectorizer = CountVectorizer() # basic word count input = corpus, stop_words = \"english\"\n",
        "  texts = vectorizer.fit_transform(docs).toarray()   #vectorizer.fit(docs), basic_data = vectorizer.transform(docs), basic_data.toarray()   \n",
        "  vocab = vectorizer.vocabulary_ #this is a dictionary of words with word as key and indices number as values\n",
        "  vocab = sorted(vocab.items(), key = lambda x: x[1]) #returns tuple (key, value)\n",
        "  vocab = [v[0] for v in vocab]\n",
        "  \n",
        "  return texts,classes,vocab\n",
        "\n",
        "# This function can add other features you want that help classification\n",
        "# accuracy, such as bigrams, word prefixes and suffixes, etc.\n",
        "def createFancyFeatures(corpus):\n",
        "  X = [dict['text'] for dict in corpus] #list of reviews Or map(lambda dict: dict['text'], corpus)\n",
        "  classes = [ dict['class'] for dict in corpus] #list of correct classes\n",
        "\n",
        "  #Pre-process the data\n",
        "  docs = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stopw = stopwords.words('english')\n",
        "\n",
        "  for rev in range(0, len(X)):\n",
        "    doc = re.sub(r'[\\W_]', ' ', str(X[rev])) #removes '_' around words as well\n",
        "    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
        "    doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
        "    doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n",
        "    doc = re.sub(r'^b\\s+', '', doc)\n",
        "    doc = doc.lower() \n",
        "    doc = re.sub(r'[0-9]+', '', doc)\n",
        "\n",
        "    #Lemmatization\n",
        "    doc = doc.split()\n",
        "    doc = [lemmatizer.lemmatize(word) for word in doc] # noun PoS works better than verb and adjective\n",
        "    doc = ' '.join(doc)\n",
        "\n",
        "    docs.append(doc)\n",
        "  #remove stop words\n",
        "  #remove more frequent words across all the reviews (remove if the word is present on at least 80% of the reviews), and words which have appeared less than 3 times \n",
        " \n",
        "  vectorizer = CountVectorizer(stop_words = \"english\", max_df=0.8, min_df=3) #ngram_range=(1,2)\n",
        "  texts = vectorizer.fit_transform(docs).toarray()  \n",
        "  vocab = vectorizer.vocabulary_ \n",
        "  vocab = sorted(vocab.items(), key = lambda x: x[1])\n",
        "  vocab = [v[0] for v in vocab]\n",
        "  return texts,classes,vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQ75CVcaskb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#another feature extraction using TF-IDF representation\n",
        "def createFancyFeatures_1(corpus):\n",
        "  X = [dict['text'] for dict in corpus] #list of reviews Or map(lambda dict: dict['text'], corpus)\n",
        "  classes = [ dict['class'] for dict in corpus] #list of correct classes\n",
        "\n",
        "  #Pre-process the data\n",
        "  docs = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stopw = stopwords.words('english')\n",
        "\n",
        "  for rev in range(0, len(X)):\n",
        "    doc = re.sub(r'[\\W_]', ' ', str(X[rev])) #removes _ around words as well\n",
        "    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
        "    doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
        "    doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n",
        "    doc = re.sub(r'^b\\s+', '', doc)\n",
        "    doc = doc.lower() \n",
        "    doc = re.sub(r'[0-9]+', '', doc)\n",
        "\n",
        "    #Lemmatization\n",
        "    doc = doc.split()\n",
        "    doc = [lemmatizer.lemmatize(word) for word in doc] \n",
        "    #doc = [lemmatizer.lemmatize(word, pos='v') for word in doc] \n",
        "    #doc = [lemmatizer.lemmatize(word, pos='a') for word in doc] \n",
        "    doc = ' '.join(doc)\n",
        "\n",
        "    docs.append(doc)\n",
        "  #remove stop words\n",
        "  #use TF_IDF and remove  \n",
        " \n",
        "  vectorizer = TfidfVectorizer(stop_words = \"english\", ngram_range=(1,2), max_df=0.7, min_df=5)\n",
        "  texts = vectorizer.fit_transform(docs).toarray()  \n",
        "  vocab = vectorizer.vocabulary_ \n",
        "  vocab = sorted(vocab.items(), key = lambda x: x[1])\n",
        "  vocab = [v[0] for v in vocab]\n",
        "  return texts,classes,vocab "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEnmb4M3Ps-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stop = stopwords.words('english')\n",
        "#these stopwords, contain's 'not' words which are important for bad review in bigram hence not using this explicit list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfTBqBltXe7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#given a numpy matrix representation of the features for the training set, the \n",
        "# vector of true classes for each example, and the vocabulary as described \n",
        "# above, this computes the accuracy of the model using leave one out cross \n",
        "# validation and reports the most indicative features for each class\n",
        "\n",
        "def evaluateModel(X,y,vocab,penalty=\"l1\"):\n",
        "  #create and fit the model\n",
        "  model = LogisticRegression(penalty=penalty,solver=\"liblinear\")\n",
        "  results = cross_validate(model,X,y,cv=KFold(n_splits=10, shuffle=True, random_state=1))\n",
        "  \n",
        "  #determine the average accuracy\n",
        "  scores = results[\"test_score\"]\n",
        "  avg_score = sum(scores)/len(scores)\n",
        "  \n",
        "  #determine the most informative features\n",
        "  # this requires us to fit the model to everything, because we need a\n",
        "  # single model to draw coefficients from, rather than 26\n",
        "  model.fit(X,y)\n",
        "  class0_weight_sorted = model.coef_[0, :].argsort()\n",
        "  class1_weight_sorted = (-model.coef_[0, :]).argsort()\n",
        "\n",
        "  termsToTake = 20\n",
        "  class0_indicators = [vocab[i] for i in class0_weight_sorted[:termsToTake]]\n",
        "  class1_indicators = [vocab[i] for i in class1_weight_sorted[:termsToTake]]\n",
        "\n",
        "  if model.classes_[0] == \"pos\":\n",
        "    return avg_score,class0_indicators,class1_indicators\n",
        "  else:\n",
        "    return avg_score,class1_indicators,class0_indicators\n",
        "\n",
        "def runEvaluation(X,y,vocab):\n",
        "  print(\"----------L1 Norm-----------\")\n",
        "  avg_score,pos_indicators,neg_indicators = evaluateModel(X,y,vocab,\"l1\")\n",
        "  print(\"The model's average accuracy is %f\"%avg_score)\n",
        "  print(\"The most informative terms for pos are: %s\"%pos_indicators)\n",
        "  print(\"The most informative terms for neg are: %s\"%neg_indicators)\n",
        "  #this call will fit a model with L2 normalization\n",
        "  print(\"----------L2 Norm-----------\")\n",
        "  avg_score,pos_indicators,neg_indicators = evaluateModel(X,y,vocab,\"l2\")\n",
        "  print(\"The model's average accuracy is %f\"%avg_score)\n",
        "  print(\"The most informative terms for pos are: %s\"%pos_indicators)\n",
        "  print(\"The most informative terms for neg are: %s\"%neg_indicators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWWq5VgmECKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = readReviews()\n",
        "#corpus[1:5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72OUet02TjBo",
        "colab_type": "text"
      },
      "source": [
        "Run the following to train and evaluate two models using basic features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IpJ7PKjvc8I",
        "colab_type": "code",
        "outputId": "e1843c22-8333-4521-c99e-3f406f7a0ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "X,y,vocab = createBasicFeatures(corpus)\n",
        "runEvaluation(X, y, vocab)\n",
        "print(\"size of vocabulary: {}\".format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------L1 Norm-----------\n",
            "The model's average accuracy is 0.826000\n",
            "The most informative terms for pos are: ['flaws', 'memorable', 'terrific', 'perfectly', 'masterpiece', 'edge', 'enjoyable', 'gas', 'using', 'sherri', 'excellent', 'overall', 'fun', 'command', 'holds', 'quite', 'follows', 'different', 'allows', 'solid']\n",
            "The most informative terms for neg are: ['waste', 'mess', 'ridiculous', 'lame', 'headed', 'worst', 'cheap', 'unfortunately', 'awful', 'write', 'tedious', 'boring', 'iii', 'jesse', 'superior', 'poor', 'bad', 'terrible', 'flat', 'looks']\n",
            "----------L2 Norm-----------\n",
            "The model's average accuracy is 0.832500\n",
            "The most informative terms for pos are: ['fun', 'great', 'back', 'quite', 'well', 'excellent', 'perfectly', 'memorable', 'overall', 'american', 'job', 'terrific', 'pulp', 'seen', 'yet', 'true', 'performances', 'bit', 'husband', 'others']\n",
            "The most informative terms for neg are: ['bad', 'unfortunately', 'worst', 'waste', 'nothing', 'script', 'only', 'boring', 'awful', 'plot', 'poor', 'reason', 'looks', 'mess', 'supposed', 'lame', 'women', 'anyway', 'any', 'should']\n",
            "size of vocabulary: 38890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO4r3p_ggo9u",
        "colab_type": "text"
      },
      "source": [
        "Form the above top 20 words/features, we can see that there are numbers which does not add any information to review considering bag of words model. There are also stopwords such as 'any','yet', 'only', 'should', which should be removed. We can also see few words which corresponds to plot of teh movie, hence will be removing most frequent and very less frequent word to create advanced features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sjxLL2PTrJi",
        "colab_type": "text"
      },
      "source": [
        "Run the following to train and evaluate two models using extended features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHudrPb5NPY",
        "colab_type": "code",
        "outputId": "ebf3d9b6-7fc8-4cc3-e1f8-bb488df26472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "X,y,vocab = createFancyFeatures(corpus)\n",
        "runEvaluation(X, y, vocab)\n",
        "print(\"size of vocabulary: {}\".format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------L1 Norm-----------\n",
            "The model's average accuracy is 0.816500\n",
            "The most informative terms for pos are: ['memorable', 'overall', 'excellent', 'terrific', 'fantastic', 'equally', 'daylight', 'deserves', 'succeeds', 'wonderfully', 'hilarious', 'belief', 'perfectly', 'command', 'sullivan', 'different', 'fun', 'flaw', 'definitely', 'pace']\n",
            "The most informative terms for neg are: ['ridiculous', 'worst', 'waste', 'headed', 'designed', 'supposed', 'awful', 'lame', 'mess', 'wasted', 'poor', 'forward', 'unfortunately', 'guess', 'cheap', 'terrible', 'saved', 'potential', 'bad', 'write']\n",
            "----------L2 Norm-----------\n",
            "The model's average accuracy is 0.842000\n",
            "The most informative terms for pos are: ['fun', 'great', 'overall', 'memorable', 'different', 'excellent', 'hilarious', 'quite', 'perfectly', 'matrix', 'terrific', 'true', 'definitely', 'entertaining', 'enjoyed', 'performance', 'pace', 'follows', 'job', 'enjoyable']\n",
            "The most informative terms for neg are: ['bad', 'worst', 'unfortunately', 'supposed', 'waste', 'attempt', 'poor', 'awful', 'boring', 'plot', 'mess', 'fall', 'ridiculous', 'lame', 'stupid', 'cheap', 'reason', 'wasted', 'guess', 'terrible']\n",
            "size of vocabulary: 16197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN4Uvb32dTAv",
        "colab_type": "text"
      },
      "source": [
        "bi-gram model does not perform better than uni-gram model, hence using uni-gram representation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDQ-RaXlUCxL",
        "colab_type": "code",
        "outputId": "a5c0068d-5407-4548-baeb-a40315d6bacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "X,y,vocab = createFancyFeatures_1(corpus)\n",
        "runEvaluation(X, y, vocab)\n",
        "print(\"size of vocabulary: {}\".format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------L1 Norm-----------\n",
            "The model's average accuracy is 0.735500\n",
            "The most informative terms for pos are: ['great', 'performance', 'life', 'war', 'perfectly', 'seen', 'quite', 'excellent', 'jackie', 'best', 'perfect', 'family', 'world', 'overall', 'fun', 'different', 'titanic', 'american', 'mulan', 'hilarious']\n",
            "The most informative terms for neg are: ['bad', 'worst', 'boring', 'supposed', 'attempt', 'waste', 'plot', 'minute', 'stupid', 'mess', 'unfortunately', 'dull', 'ridiculous', 'script', 'awful', 'joke', 'wasted', 'tv', 'harry', 'look']\n",
            "----------L2 Norm-----------\n",
            "The model's average accuracy is 0.834000\n",
            "The most informative terms for pos are: ['life', 'great', 'performance', 'war', 'truman', 'family', 'jackie', 'excellent', 'world', 'best', 'quite', 'perfect', 'mulan', 'american', 'cameron', 'perfectly', 'fun', 'hilarious', 'titanic', 'seen']\n",
            "The most informative terms for neg are: ['bad', 'worst', 'plot', 'boring', 'supposed', 'stupid', 'attempt', 'waste', 'minute', 'script', 'unfortunately', 'poor', 'harry', 'awful', 'mess', 'joke', 'look', 'ridiculous', 'dull', 'wasted']\n",
            "size of vocabulary: 20046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0JDZc6YcQhI",
        "colab_type": "text"
      },
      "source": [
        "Using TF-IDF represntation (with or with out n-gram ) does not perform as good as word-count representation "
      ]
    }
  ]
}