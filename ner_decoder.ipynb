{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY0LJOIT_cG",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a Viterbi Decoder and Evaluation for Sequence Labeling\n",
        "\n",
        "In this assignment, you will build a Viterbi decoder for an LSTM named-entity recognition model. As we mentioned in class, recurrent and bidirectional recurrent neural networks, of which LSTMs are the most common examples, can be used to perform sequence labeling. Although these models encode information from the surrounding words in order to make predictions, there are no \"hard\" constraints on what tags can appear where.\n",
        "\n",
        "There hard constraints are particularly important for tasks that label spans of more than one token. The most common example of a span-labeling task is named-entity recognition (NER). As described in Eisenstein, Jurafksy & Martin, and other texts, the goal of NER is to label spans of one or more words as _mentions_ of an _entity_, such as a person, location, organization, etc.\n",
        "\n",
        "The most common approach to NER is to reduce it to a sequence-labeling task, where each token in the input is labeled either with an `O`, if it is \"outside\" any named-entity span, or with `B-TYPE`, if it is the first token in an entity of type `TYPE`, or with `I-TYPE`, if it is the second or later token in an entity of type `TYPE`. Distinguishing between the first and later tokens of an entity allow us to identify distinct entity spans even when they are adjacent.\n",
        "\n",
        "Common values of `TYPE` include `PER` for person, `LOC` for location, `DATE` for date, and so on. In the dataset we load below, there are 17 distinct types.\n",
        "\n",
        "The span-labeling scheme just described implies that the labels on tokens must obey certain constraints: the tag `I-PER` must follow either `B-PER` or another `I-PER`. It cannot follow `O`, `B-LOC`, or `I-LOC`, i.e., a tag for a different entity type. By themselves, LSTMs or bidirectional LSTMs cannot directly enforce these constraints. This is one reason why conditional random fields (CRFs), which _can_ enforce these constraints, are often layered on top of these recurrent models.\n",
        "\n",
        "In this assignment, you will implement the simplest possible CRF: a CRF so simple that it does not require any training. Rather, it will assign weight 1 to any sequence of tags that obeys the constraints and weight 0 to any sequence of tags that violates them. The inputs to the CRF, which are analogous to the emission probabilities in an HMM, will come from an LSTM.\n",
        "\n",
        "But first, in order to test your decoder, you will also implement some functions to evaluate the output of an NER system according to two metrics:\n",
        "1. You will count the number of _violations_ of the NER label constraints, i.e., how many times `I-TYPE` follows `O` or a tag of a different type or occurs at the beginning of a sentence. This number will be greater than 0 in the raw LSTM output, but should be 0 for your CRF output.\n",
        "1. You will compute the _span-level_ precision, recall, and F1 of NER output. Although the baseline LSTM was trained to achieve high _token-level_ accuracy, this metric can be misleadingly high, since so many tokens are correctly labeled `O`. In other words, what proportion of spans predicted by the model line up exactly with spans in the gold standard, and what proportion of spans in the gold standard were predicted by the model? For more, see the original task definition: https://www.aclweb.org/anthology/W03-0419/.\n",
        "\n",
        "We start with loading some code and data and the describe your tasks in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnn49QEU_Ik",
        "colab_type": "text"
      },
      "source": [
        "## Set Up Dependencies and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJINX1MwOLBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install spacy==2.1.0 allennlp\n",
        "!pip install --upgrade spacy allennlp\n",
        "import spacy\n",
        "print(spacy.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zJfaIlJ2bv",
        "colab_type": "code",
        "outputId": "dc3ed6c6-7712-41f7-d6d4-52c5f9ffad2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from typing import Iterator, List, Dict\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.dataset_readers import conll2003\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f36263594d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo16Ko0Gchxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmTagger(Model):\n",
        "  def __init__(self,\n",
        "               word_embeddings: TextFieldEmbedder,\n",
        "               encoder: Seq2SeqEncoder,\n",
        "               vocab: Vocabulary) -> None:\n",
        "    super().__init__(vocab)\n",
        "    self.word_embeddings = word_embeddings\n",
        "    self.encoder = encoder\n",
        "    self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(self,\n",
        "              tokens: Dict[str, torch.Tensor],\n",
        "              metadata,\n",
        "              tags: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "    mask = get_text_field_mask(tokens)\n",
        "    embeddings = self.word_embeddings(tokens)\n",
        "    encoder_out = self.encoder(embeddings, mask)\n",
        "    tag_logits = self.hidden2tag(encoder_out)\n",
        "    output = {\"tag_logits\": tag_logits}\n",
        "    if tags is not None:\n",
        "      self.accuracy(tag_logits, tags, mask)\n",
        "      output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, tags, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {\"accuracy\": self.accuracy.get_metric(reset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdKvPftVVLt",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOVVZslKm3N",
        "colab_type": "code",
        "outputId": "84a78002-0032-4f09-9354-814112b07451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "reader = conll2003.Conll2003DatasetReader()\n",
        "train_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.train.ner.sample'))\n",
        "validation_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.development.ner.sample'))\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159377B [00:00, 1953327.61B/s]\n",
            "562it [00:00, 4708.35it/s]\n",
            "8366B [00:00, 13211425.93B/s]\n",
            "23it [00:00, 3474.11it/s]\n",
            "100%|██████████| 585/585 [00:00<00:00, 47950.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tlkFZsprEgq",
        "colab_type": "code",
        "outputId": "d08048b7-9cdd-4906-ae1e-9f611e04d297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocabulary with namespaces:  tokens, Size: 1931 || labels, Size: 34 || Non Padded Namespaces: {'*tags', '*labels'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpg2Udr-Vnwm",
        "colab_type": "text"
      },
      "source": [
        "## Define and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kDQQBMywdKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=False, batch_first=True))\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=100,\n",
        "                  cuda_device=cuda_device)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bzK5lZasenW",
        "colab_type": "text"
      },
      "source": [
        "{'best_epoch': 98,\n",
        "\n",
        " 'best_validation_accuracy': 0.8795918367346939,\n",
        "\n",
        " 'best_validation_loss': 0.36728941490097594,\n",
        " \n",
        " 'epoch': 99,\n",
        " \n",
        " 'peak_cpu_memory_MB': 2600.008,\n",
        " \n",
        " 'peak_gpu_0_memory_MB': 541,\n",
        " \n",
        " 'training_accuracy': 0.9259612329202415,\n",
        " \n",
        " 'training_cpu_memory_MB': 2600.008,\n",
        " \n",
        " 'training_duration': '0:02:16.681880',\n",
        " \n",
        " 'training_epochs': 99,\n",
        " \n",
        " 'training_gpu_0_memory_MB': 541,\n",
        " \n",
        " 'training_loss': 0.19139084464135445,\n",
        " \n",
        " 'training_start_epoch': 0,\n",
        " \n",
        " 'validation_accuracy': 0.8775510204081632,\n",
        " \n",
        " 'validation_loss': 0.3792670750020382}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwN6ctqVV0tf",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDs_UdIeuFz",
        "colab_type": "text"
      },
      "source": [
        "The simple code below creators a `predictor` object that applies the model to an input example and then loops over the examples in the validation set, printing out the input token, gold-standard output, and model output. You can see from these methods how to access data and model outputs for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bE4fmLik08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
        "\n",
        "def tag_sentence(s):\n",
        "  tag_ids = np.argmax(predictor.predict_instance(s)['tag_logits'], axis=-1)\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)\n",
        "\n",
        "baseline_output = [tag_sentence(i) for i in validation_dataset]\n",
        "training_output = [tag_sentence(i) for i in train_dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIJ0Z1WswCiO",
        "colab_type": "code",
        "outputId": "75738ad1-b110-4ef7-f815-4242955e78e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#(token, tag, predicted tag)\n",
        "print(baseline_output)\n",
        "len(baseline_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(With, 'O', 'O'), (a, 'O', 'O'), (wave, 'O', 'O'), (of, 'O', 'O'), (his, 'O', 'O'), (hand, 'O', 'O'), (,, 'O', 'O'), (Peng, 'B-PERSON', 'B-PERSON'), (Dehuai, 'I-PERSON', 'I-ORG'), (said, 'O', 'O'), (that, 'O', 'O'), (despite, 'O', 'O'), (being, 'O', 'O'), (over, 'O', 'O'), (100, 'B-CARDINAL', 'B-CARDINAL'), (regiments, 'O', 'O'), (,, 'O', 'O'), (let, 'O', 'O'), ('s, 'O', 'O'), (call, 'O', 'O'), (this, 'O', 'O'), (campaign, 'O', 'O'), (the, 'B-EVENT', 'O'), (Hundred, 'I-EVENT', 'I-EVENT'), (Regiments, 'I-EVENT', 'I-EVENT'), (Offensive, 'I-EVENT', 'I-EVENT'), (., 'O', 'O')], [(That, 'B-TIME', 'O'), (afternoon, 'I-TIME', 'I-DATE'), (,, 'O', 'O'), (Peng, 'B-PERSON', 'B-PERSON'), (Dehuai, 'I-PERSON', 'I-DATE'), (first, 'B-ORDINAL', 'I-DATE'), (used, 'O', 'O'), (the, 'O', 'O'), (name, 'O', 'O'), (Hundred, 'B-EVENT', 'I-EVENT'), (Regiments, 'I-EVENT', 'I-EVENT'), (Offensive, 'I-EVENT', 'I-EVENT'), (in, 'O', 'O'), (telegrams, 'O', 'O'), (sent, 'O', 'O'), (to, 'O', 'O'), (various, 'O', 'O'), (military, 'O', 'O'), (corps, 'O', 'O'), (and, 'O', 'O'), (the, 'B-ORG', 'O'), (Central, 'I-ORG', 'I-ORG'), (Military, 'I-ORG', 'I-ORG'), (Commission, 'I-ORG', 'I-ORG'), (., 'O', 'O')], [(A, 'O', 'O'), (total, 'O', 'O'), (of, 'O', 'O'), (105, 'B-CARDINAL', 'B-CARDINAL'), (regiments, 'O', 'O'), (participated, 'O', 'O'), (in, 'O', 'O'), (the, 'B-EVENT', 'O'), (Hundred, 'I-EVENT', 'I-EVENT'), (Regiments, 'I-EVENT', 'I-EVENT'), (Offensive, 'I-EVENT', 'I-EVENT'), (., 'O', 'O')], [(Well, 'O', 'O'), (,, 'O', 'O'), (it, 'O', 'O'), (was, 'O', 'O'), (a, 'O', 'O'), (force, 'O', 'O'), (of, 'O', 'O'), (more, 'B-CARDINAL', 'O'), (than, 'I-CARDINAL', 'I-DATE'), (200,000, 'I-CARDINAL', 'I-DATE'), (soldiers, 'O', 'O'), (., 'O', 'O')], [(In, 'O', 'O'), (addition, 'O', 'O'), (to, 'O', 'O'), (that, 'O', 'O'), (,, 'O', 'O'), (er, 'O', 'O'), (,, 'O', 'O'), (no, 'B-CARDINAL', 'O'), (less, 'I-CARDINAL', 'I-DATE'), (than, 'I-CARDINAL', 'I-DATE'), (400,000, 'I-CARDINAL', 'I-DATE'), (militiamen, 'O', 'O'), (,, 'O', 'O'), (guerrillas, 'O', 'O'), (,, 'O', 'O'), (and, 'O', 'O'), (civilians, 'O', 'O'), (,, 'O', 'O'), (er, 'O', 'O'), (,, 'O', 'O'), (voluntarily, 'O', 'O'), (participated, 'O', 'O'), (in, 'O', 'O'), (the, 'O', 'O'), (offensive, 'O', 'O'), (., 'O', 'O')], [(So, 'O', 'O'), (,, 'O', 'O'), (there, 'O', 'O'), (were, 'O', 'O'), (probably, 'O', 'O'), (600,000, 'B-CARDINAL', 'B-CARDINAL'), (-, 'I-CARDINAL', 'I-DATE'), (plus, 'I-CARDINAL', 'I-DATE'), (soldiers, 'O', 'O'), (and, 'O', 'O'), (civilians, 'O', 'O'), (., 'O', 'O')], [(The, 'B-EVENT', 'O'), (Hundred, 'I-EVENT', 'I-EVENT'), (Regiments, 'I-EVENT', 'I-EVENT'), (Offensive, 'I-EVENT', 'I-EVENT'), (dealt, 'O', 'O'), (a, 'O', 'O'), (devastating, 'O', 'O'), (blow, 'O', 'O'), (to, 'O', 'O'), (the, 'O', 'O'), (enemy, 'O', 'O'), (at, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-CARDINAL'), (Mountain, 'I-LOC', 'I-EVENT'), (., 'O', 'O')], [(Our, 'O', 'O'), (troops, 'O', 'O'), (were, 'O', 'O'), (strong, 'O', 'O'), (,, 'O', 'O'), (troops, 'O', 'O'), (were, 'O', 'O'), (strong, 'O', 'O'), (., 'O', 'O')], [(Er, 'O', 'O'), (,, 'O', 'O'), (and, 'O', 'O'), (those, 'O', 'O'), (in, 'O', 'O'), (charge, 'O', 'O'), (of, 'O', 'O'), (logistics, 'O', 'O'), (and, 'O', 'O'), (support, 'O', 'O'), (were, 'O', 'O'), (able, 'O', 'O'), (to, 'O', 'O'), (follow, 'O', 'O'), (in, 'O', 'O'), (time, 'O', 'O'), (., 'O', 'O')], [(Er, 'O', 'O'), (,, 'O', 'O'), (,, 'O', 'O'), (er, 'O', 'O'), (,, 'O', 'O'), (the, 'O', 'O'), (anti-Japanese, 'B-NORP', 'B-NORP'), (vanguard, 'O', 'O'), (team, 'O', 'O'), (was, 'O', 'O'), (composed, 'O', 'O'), (of, 'O', 'O'), (all, 'O', 'O'), (young, 'O', 'O'), (people, 'O', 'O'), (in, 'O', 'O'), (their, 'O', 'O'), (20s, 'B-DATE', 'B-CARDINAL'), (;, 'O', 'O'), (all, 'O', 'O'), (young, 'O', 'O'), (people, 'O', 'O'), (,, 'O', 'O'), (all, 'O', 'O'), (in, 'O', 'O'), (their, 'O', 'O'), (,, 'O', 'O'), (20s, 'B-DATE', 'B-CARDINAL'), (., 'O', 'O')], [(This, 'O', 'O'), (is, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-CARDINAL'), (Mountain, 'I-LOC', 'I-EVENT'), (,, 'O', 'O'), (situated, 'O', 'O'), (in, 'O', 'O'), (Yangquan, 'B-GPE', 'B-GPE'), (,, 'I-GPE', 'O'), (Shanxi, 'I-GPE', 'I-DATE'), (,, 'O', 'O'), (a, 'O', 'O'), (strategic, 'O', 'O'), (passage, 'O', 'O'), (of, 'O', 'O'), (the, 'B-FAC', 'B-ORG'), (Zhengtai, 'I-FAC', 'I-ORG'), (Railway, 'I-FAC', 'I-ORG'), (back, 'O', 'O'), (then, 'O', 'O'), (., 'O', 'O')], [(On, 'O', 'O'), (August, 'B-DATE', 'O'), (21, 'I-DATE', 'I-DATE'), (,, 'I-DATE', 'O'), (1940, 'I-DATE', 'I-DATE'), (,, 'O', 'O'), (in, 'O', 'O'), (order, 'O', 'O'), (to, 'O', 'O'), (pin, 'O', 'O'), (down, 'O', 'O'), (Japanese, 'B-NORP', 'B-NORP'), (troops, 'O', 'O'), (stationed, 'O', 'O'), (in, 'O', 'O'), (Yangquan, 'B-GPE', 'B-GPE'), (and, 'O', 'O'), (to, 'O', 'O'), (cover, 'O', 'O'), (for, 'O', 'O'), (militiamen, 'O', 'O'), (sabotaging, 'O', 'O'), (the, 'O', 'O'), (western, 'O', 'O'), (section, 'O', 'O'), (of, 'O', 'O'), (the, 'B-FAC', 'B-ORG'), (Zhengtai, 'I-FAC', 'I-ORG'), (Railway, 'I-FAC', 'I-ORG'), (,, 'O', 'O'), (the, 'B-ORG', 'O'), (129th, 'I-ORG', 'I-ORG'), (Division, 'I-ORG', 'I-ORG'), (of, 'O', 'O'), (the, 'B-ORG', 'I-ORG'), (Eighth, 'I-ORG', 'I-ORG'), (Route, 'I-ORG', 'I-ORG'), (Army, 'I-ORG', 'I-ORG'), (ordered, 'O', 'O'), (the, 'O', 'O'), (main, 'O', 'O'), (regiment, 'O', 'O'), (of, 'O', 'O'), (its, 'O', 'O'), (385th, 'B-ORG', 'B-CARDINAL'), (Brigade, 'I-ORG', 'I-ORG'), (,, 'O', 'O'), (the, 'B-ORG', 'O'), (14th, 'I-ORG', 'I-ORG'), (Regiment, 'I-ORG', 'I-ORG'), (led, 'O', 'O'), (by, 'O', 'O'), (Brigade, 'O', 'O'), (Commander, 'O', 'O'), (Chen, 'B-PERSON', 'B-PERSON'), (Xilian, 'I-PERSON', 'I-ORG'), (,, 'O', 'O'), (to, 'O', 'O'), (head, 'O', 'O'), (directly, 'O', 'O'), (for, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-PERSON'), (Mountain, 'I-LOC', 'I-ORG'), (to, 'O', 'O'), (seize, 'O', 'O'), (the, 'O', 'O'), (high, 'O', 'O'), (ground, 'O', 'O'), (., 'O', 'O')], [(Like, 'O', 'O'), (a, 'O', 'O'), (sharp, 'O', 'O'), (sword, 'O', 'O'), (,, 'O', 'O'), (civilians, 'O', 'O'), (in, 'O', 'O'), (the, 'O', 'O'), (western, 'O', 'O'), (section, 'O', 'O'), (broke, 'O', 'O'), (through, 'O', 'O'), (the, 'O', 'O'), (strategic, 'O', 'O'), (passage, 'O', 'O'), (., 'O', 'O')], [(Frustrated, 'O', 'O'), (and, 'O', 'O'), (angry, 'O', 'O'), (,, 'O', 'O'), (the, 'O', 'O'), (Japanese, 'B-NORP', 'B-NORP'), (army, 'O', 'O'), (immediately, 'O', 'O'), (deployed, 'O', 'O'), (a, 'O', 'O'), (large, 'O', 'O'), (number, 'O', 'O'), (of, 'O', 'O'), (troops, 'O', 'O'), (in, 'O', 'O'), (an, 'O', 'O'), (attempt, 'O', 'O'), (to, 'O', 'O'), (retake, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-PERSON'), (Mountain, 'I-LOC', 'I-ORG'), (., 'O', 'O')], [(To, 'O', 'O'), (the, 'O', 'O'), (enemy, 'O', 'O'), ('s, 'O', 'O'), (complete, 'O', 'O'), (surprise, 'O', 'O'), (,, 'O', 'O'), (desolate, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-PERSON'), (Mountain, 'I-LOC', 'I-ORG'), (had, 'O', 'O'), (been, 'O', 'O'), (turned, 'O', 'O'), (into, 'O', 'O'), (a, 'O', 'O'), (strong, 'O', 'O'), (combat, 'O', 'O'), (fortress, 'O', 'O'), (by, 'O', 'O'), (our, 'O', 'O'), (troops, 'O', 'O'), (in, 'O', 'O'), (less, 'B-TIME', 'B-PERSON'), (than, 'I-TIME', 'I-DATE'), (a, 'I-TIME', 'I-DATE'), (night, 'I-TIME', 'I-DATE'), (., 'O', 'O')], [(Like, 'O', 'O'), (a, 'O', 'O'), (lion, 'O', 'O'), (with, 'O', 'O'), (its, 'O', 'O'), (mouth, 'O', 'O'), (wide, 'O', 'O'), (open, 'O', 'O'), (,, 'O', 'O'), (towering, 'O', 'O'), (and, 'O', 'O'), (rugged, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-PERSON'), (Mountain, 'I-LOC', 'I-ORG'), (was, 'O', 'O'), (constantly, 'O', 'O'), (waiting, 'O', 'O'), (for, 'O', 'O'), (any, 'O', 'O'), (invaders, 'O', 'O'), (who, 'O', 'O'), (dared, 'O', 'O'), (come, 'O', 'O'), (close, 'O', 'O'), (., 'O', 'O')], [(The, 'B-TIME', 'O'), (night, 'I-TIME', 'I-DATE'), (before, 'I-TIME', 'I-DATE'), (,, 'O', 'O'), (we, 'O', 'O'), (destroyed, 'O', 'O'), (the, 'O', 'O'), (roads, 'O', 'O'), (and, 'O', 'O'), (cut, 'O', 'O'), (off, 'O', 'O'), (all, 'O', 'O'), (electrical, 'O', 'O'), (and, 'O', 'O'), (railway, 'O', 'O'), (lines, 'O', 'O'), (,, 'O', 'O'), (making, 'O', 'O'), (it, 'O', 'O'), (impossible, 'O', 'O'), (for, 'O', 'O'), (it, 'O', 'O'), (to, 'O', 'O'), (communicate, 'O', 'O'), (., 'O', 'O')], [(It, 'O', 'O'), (did, 'O', 'O'), (not, 'O', 'O'), (know, 'O', 'O'), (how, 'O', 'O'), (many, 'O', 'O'), (of, 'O', 'O'), (us, 'O', 'O'), (went, 'O', 'O'), (to, 'O', 'O'), (destroy, 'O', 'O'), (the, 'O', 'O'), (roads, 'O', 'O'), (,, 'O', 'O'), (whether, 'O', 'O'), (it, 'O', 'O'), (was, 'O', 'O'), (the, 'B-ORG', 'O'), (Eighth, 'I-ORG', 'I-ORG'), (Route, 'I-ORG', 'I-ORG'), (Army, 'I-ORG', 'I-ORG'), (or, 'O', 'O'), (not, 'O', 'O'), (,, 'O', 'O'), (the, 'O', 'O'), (regulars, 'O', 'O'), (,, 'O', 'O'), (or, 'O', 'O'), (others, 'O', 'O'), (., 'O', 'O')], [(It, 'O', 'O'), (did, 'O', 'O'), (not, 'O', 'O'), (know, 'O', 'O'), (,, 'O', 'O'), (right, 'O', 'O'), (?, 'O', 'O')], [(After, 'O', 'O'), (all, 'O', 'O'), (,, 'O', 'O'), (its, 'O', 'O'), (telephones, 'O', 'O'), (had, 'O', 'O'), (stopped, 'O', 'O'), (working, 'O', 'O'), (., 'O', 'O')], [(Yeah, 'O', 'O'), (,, 'O', 'O'), (the, 'O', 'O'), (roads, 'O', 'O'), (had, 'O', 'O'), (been, 'O', 'O'), (cut, 'O', 'O'), (off, 'O', 'O'), (,, 'O', 'O'), (with, 'O', 'O'), (its, 'O', 'O'), (vehicles, 'O', 'O'), (unable, 'O', 'O'), (to, 'O', 'O'), (drive, 'O', 'O'), (back, 'O', 'O'), (and, 'O', 'O'), (forth, 'O', 'O'), (., 'O', 'O')], [(The, 'O', 'O'), (following, 'O', 'O'), (seven, 'B-DATE', 'B-CARDINAL'), (days, 'I-DATE', 'I-DATE'), (and, 'I-DATE', 'O'), (nights, 'I-DATE', 'I-DATE'), (were, 'O', 'O'), (extremely, 'O', 'O'), (tough, 'O', 'O'), (., 'O', 'O')], [(Starting, 'O', 'O'), (from, 'O', 'O'), (the, 'B-DATE', 'O'), (23rd, 'I-DATE', 'I-DATE'), (,, 'O', 'O'), (the, 'O', 'O'), (Japanese, 'B-NORP', 'B-NORP'), (army, 'O', 'O'), (in, 'O', 'O'), (Yangquan, 'B-GPE', 'B-GPE'), (,, 'O', 'O'), (backed, 'O', 'O'), (by, 'O', 'O'), (helicopters, 'O', 'O'), (,, 'O', 'O'), (launched, 'O', 'O'), (nonstop, 'O', 'O'), (fierce, 'O', 'O'), (attacks, 'O', 'O'), (on, 'O', 'O'), (Shi'nao, 'B-LOC', 'B-PERSON'), (Mountain, 'I-LOC', 'I-EVENT'), (using, 'O', 'O'), (chemical, 'O', 'O'), (weapons, 'O', 'O'), (., 'O', 'O')]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyKLiqK7w58f",
        "colab_type": "code",
        "outputId": "30d5a8cd-6b76-4603-ff74-a172c51a4fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(baseline_output[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpYMx7RVfCyT",
        "colab_type": "text"
      },
      "source": [
        "Now, you can implement two evaluation functions: `violations` and `span_stats`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgfks_Cmy3eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P042A2Ofg3wa",
        "colab_type": "code",
        "outputId": "cff9ce15-1837-44ce-b26e-267213e485d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# TODO: count the number of NER label violations,\n",
        "# such as O followed by I-TYPE or B-TYPE followed by\n",
        "# I-OTHER_TYPE\n",
        "# Take tagger output as input\n",
        "def violations(tagged):\n",
        "  count = 0\n",
        "  for sentence in tagged:\n",
        "    for i in range(len(sentence)-1):\n",
        "      if i == 0 and sentence[i][2].split('-')[0] == 'I': #if the starting tag itself is I-Type then it is violation\n",
        "        count += 1\n",
        "      if sentence[i][2] == 'O' and sentence[i+1][2].split('-')[0] == 'I':\n",
        "        count += 1\n",
        "      if sentence[i][2].split('-')[0] == 'B' and sentence[i+1][2].split('-')[0] == 'I':\n",
        "        if sentence[i][2].split('-')[1] != sentence[i+1][2].split('-')[1]:\n",
        "          count += 1\n",
        "    #print((sentence))\n",
        "  return count\n",
        "\n",
        "## You can check how many violations are made by the model output in predictor.\n",
        "print(\"Voilations for validation data is \", violations(baseline_output))\n",
        "print(\"Voilations for training data is \", violations(training_output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Voilations for validation data is  27\n",
            "Voilations for training data is  273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAjnvd6T2IdR",
        "colab_type": "code",
        "outputId": "d969e553-8fbb-4d62-c164-5207d743a7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# TODO: return the span-level precision, recall, and F1\n",
        "# Take tagger output as input\n",
        "def span_stats(tagged):\n",
        "  def get_entities(tagged, sys):\n",
        "   #sys =1 for goldstandard output, sys=2 for predicted output\n",
        "   spans = []  #list of spans\n",
        "   prev_tag = 'O' #initial 'O' for checking condition\n",
        "   prev_type = ''\n",
        "   begin_offset = 0\n",
        "   for sentence in tagged:\n",
        "    for i,seq in enumerate(sentence):\n",
        "      tag = seq[sys].split('-')[0]\n",
        "      try:\n",
        "        tag_type = seq[sys].split('-')[1]\n",
        "      except:\n",
        "        tag_type = seq[sys].split('-')[0] #just use 'O' as type\n",
        "      if prev_tag == 'B' and tag == 'B': \n",
        "        spans.append((prev_type, begin_offset, i-1))\n",
        "      if prev_tag == 'B' and tag == 'O': \n",
        "        spans.append((prev_type, begin_offset, i-1)) \n",
        "      if prev_tag == 'I' and tag == 'B': \n",
        "        spans.append((prev_type, begin_offset, i-1)) \n",
        "      if prev_tag == 'I' and tag == 'O': \n",
        "        spans.append((prev_type, begin_offset, i-1))\n",
        "      if prev_tag == 'B' and tag == 'I' and tag_type != prev_type: \n",
        "        spans.append((prev_type, begin_offset, i-1))\n",
        "      if prev_tag == 'I' and tag == 'I' and tag_type != prev_type: \n",
        "        spans.append((prev_type, begin_offset, i-1))\n",
        "      if tag == 'B':\n",
        "        begin_offset = i  \n",
        "      if prev_tag == 'O' and tag == 'I':\n",
        "        begin_offset = i \n",
        "      if prev_tag != 'O' and prev_type != tag_type:\n",
        "        begin_offset = i\n",
        "      prev_tag = tag\n",
        "      prev_type = tag_type\n",
        "   return spans\n",
        "  spans = get_entities(tagged, 2)\n",
        "  spans_actual = get_entities(tagged, 1)\n",
        "  #system_output = set(spans)\n",
        "  #gold_satndard = set(spans_actual)\n",
        "  #true_positive = len(system_output & gold_satndard)\n",
        "  true_positive = sum(el in spans for el in spans_actual)\n",
        "  pred_n = len(spans)\n",
        "  actual_n = len(spans_actual)\n",
        "  print(\"Number of True Positives/matches in both predicted output and gold standard:\",true_positive)\n",
        "  print(\"Number of NER spans in predicted output:\",pred_n)\n",
        "  print(\"Number of NER spans in gold standard:\", actual_n)\n",
        "  precision = true_positive/pred_n if pred_n > 0 else 0\n",
        "  recall = true_positive/actual_n if actual_n > 0 else 0\n",
        "  f1 = (2 * precision * recall) / (precision + recall) if precision+recall > 0 else 0\n",
        "  return {'precision': precision,\n",
        "          'recall': recall,\n",
        "          'f1': f1}\n",
        "\n",
        "span_stats(baseline_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of True Positives/matches in both predicted output and gold standard: 12\n",
            "Number of NER spans in predicted output: 61\n",
            "Number of NER spans in gold standard: 43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.23076923076923075,\n",
              " 'precision': 0.19672131147540983,\n",
              " 'recall': 0.27906976744186046}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob4c-YOZsBMR",
        "colab_type": "code",
        "outputId": "eafdb631-dc29-4454-95b7-b22c1dbed1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "span_stats(training_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of True Positives/matches in both predicted output and gold standard: 549\n",
            "Number of NER spans in predicted output: 1018\n",
            "Number of NER spans in gold standard: 808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.6013143483023001,\n",
              " 'precision': 0.5392927308447937,\n",
              " 'recall': 0.6794554455445545}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmezRBhml-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "labels_bio = []\n",
        "pred_bio = []\n",
        "for sentence in baseline_output:\n",
        "  tmp_1 = []\n",
        "  tmp_2 = []\n",
        "  for i,seq in enumerate(sentence):\n",
        "    #print(seq[1])\n",
        "    tmp_1.extend([seq[1]])\n",
        "    tmp_2.extend([seq[2]])\n",
        "  labels_bio.append(tmp_1)\n",
        "  #print(tmp_1)\n",
        "  pred_bio.append(tmp_2)\n",
        "#print(labels_bio) \n",
        "#pred_bio\n",
        "print(classification_report(labels_bio, pred_bio))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baC_dVskrpNU",
        "colab_type": "text"
      },
      "source": [
        "The result obtained from span_stats matches with the seqeval package results.\n",
        "\n",
        "**Validation Set Results:**\n",
        "\n",
        "F1: 0.23\n",
        "\n",
        "Precision: 0.20\n",
        "\n",
        "Recall: 0.28\n",
        "\n",
        "**Training Set Results:**\n",
        "\n",
        "F1: 0.60\n",
        "\n",
        "Precision: 0.54\n",
        "\n",
        "Recall: 0.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7-quD2hnzB",
        "colab_type": "text"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCgW9d9ohsGv",
        "colab_type": "text"
      },
      "source": [
        "Now you can finally implement the simple Viterbi decoder. The `predictor` object, when applied to an input sentence, first calculates the scores for each possible output tag for each token. See the line `predictor.predict_instance(i)['tag_logits']` in the code above.\n",
        "\n",
        "Then, you will construct a transition matrix. You can use the code below to get a list of the tags the model knows about. For a set of K tags, construct a K-by-K matrix with a log(1)=0 when a transition between a given tag pair is valid and a log(0)=-infinity otherwise.\n",
        "\n",
        "Finally, implement a Viterbi decoder that takes the predictor object and a dataset object and outputs tagged data, just like the `tag_sentence` function above. It should use the Viterbi algorithm with the (max, plus) semiring. You'll be working with sums of log probabilities instead of products of probabilties.\n",
        "\n",
        "Run your `violations` function on the output of this decoder to make sure that there are no invalid tag transitions. Also, compare the span-level metrics on `baseline_output` and your new output using your `span_stats` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlP8vZhiS68K",
        "colab_type": "code",
        "outputId": "7c3fe605-6618-41b3-d62c-e3a7cdcfba52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# This code show how to map from output vector components to labels\n",
        "print(vocab.get_index_to_token_vocabulary('labels'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'O', 1: 'B-GPE', 2: 'I-ORG', 3: 'I-DATE', 4: 'B-CARDINAL', 5: 'I-EVENT', 6: 'B-PERSON', 7: 'B-NORP', 8: 'B-DATE', 9: 'B-ORG', 10: 'B-LOC', 11: 'I-LOC', 12: 'I-FAC', 13: 'I-PERSON', 14: 'I-GPE', 15: 'I-CARDINAL', 16: 'B-EVENT', 17: 'I-TIME', 18: 'I-WORK_OF_ART', 19: 'B-ORDINAL', 20: 'B-FAC', 21: 'B-TIME', 22: 'I-LAW', 23: 'I-QUANTITY', 24: 'I-NORP', 25: 'I-MONEY', 26: 'B-MONEY', 27: 'B-WORK_OF_ART', 28: 'B-QUANTITY', 29: 'B-LAW', 30: 'B-PRODUCT', 31: 'I-PRODUCT', 32: 'B-PERCENT', 33: 'I-PERCENT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydqYqe3ZQmzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frn-F1SNDZGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = 34 #number of tags\n",
        "\n",
        "trans_score = np.array([[np.log(1) for j in range(K)] for i in range(K)]) #transition matrix\n",
        "\n",
        "for i in range(34):\n",
        "  for j in range(34):\n",
        "    if vocab.get_token_from_index(j, 'labels').split('-')[0] == 'I':\n",
        "      trans_score[i,j] = np.log(0)\n",
        "    if vocab.get_token_from_index(i, 'labels').split('-')[0] == 'O' and vocab.get_token_from_index(j, 'labels').split('-')[0] == 'I':\n",
        "      trans_score[i,j] = np.log(0) \n",
        "    if vocab.get_token_from_index(i, 'labels').split('-')[0] == 'B' and vocab.get_token_from_index(j, 'labels').split('-')[0] == 'I' and vocab.get_token_from_index(i, 'labels').split('-')[1] != vocab.get_token_from_index(j, 'labels').split('-')[1]:\n",
        "      trans_score[i,j] = np.log(0)\n",
        "    if vocab.get_token_from_index(i, 'labels').split('-')[0] == 'I' and vocab.get_token_from_index(j, 'labels').split('-')[0] == 'I' and vocab.get_token_from_index(i, 'labels').split('-')[1] != vocab.get_token_from_index(j, 'labels').split('-')[1]:\n",
        "      trans_score[i,j] = np.log(0)\n",
        " \n",
        "start_score = np.array([np.log(0) if vocab.get_token_from_index(i, 'labels').split('-')[0]=='I' else np.log(1) for i in range(34)]) #for the initial transition\n",
        "#print(vocab.get_token_from_index(i, 'labels'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_R3aH6qUeNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Each prob here is in log space\n",
        "#since ending scores/probabilities are same for all tags, only considering start/initial probability\n",
        "#initial prob or score would be -inf for I-TYPE, and for others it is 0 (log1)\n",
        "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
        "def tag_viterbi(s):\n",
        "  emission_weights = np.array(predictor.predict_instance(s)['tag_logits'])\n",
        "  N = emission_weights.shape[0] #number of tokens\n",
        "  scores = np.array([[np.log(1) for j in range(N)] for i in range(K)]) #to store scores of the trellis\n",
        "  ptr = np.array([[-1 for j in range(N)] for i in range(K)])  #to keep back pointer\n",
        "  for i in range(N):\n",
        "        if i == 0:\n",
        "            scores[:,0] = np.add(start_score, emission_weights[i])\n",
        "        else:\n",
        "            for j in range(K):\n",
        "                scores[j,i]= np.max(np.add(trans_score[:,j] , scores[:,i-1])) + emission_weights[i,j]\n",
        "                ptr[j,i] = np.argmax(np.add(trans_score[:,j] , scores[:,i-1]))\n",
        "  indx = np.argmax(scores[:, i])\n",
        "  #print(scores.shape)\n",
        "  #print(ptr.shape)\n",
        "  #print(indx)\n",
        "  tag_ids = []\n",
        "  while indx != -1:\n",
        "        tag_ids.append(indx)\n",
        "        indx = ptr[indx,i]\n",
        "        i -= 1\n",
        "  tag_ids.reverse() #reverse backpointer to get list of indices to tags\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEbxR-cmWNvO",
        "colab_type": "code",
        "outputId": "3be98012-d10f-4b61-e8f4-69445fc61906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "viterbi_output = [tag_viterbi(i) for i in validation_dataset]\n",
        "viterbi_train_output = [tag_viterbi(i) for i in train_dataset]\n",
        "print(\"Violations for validation data after viterbi is \", violations(viterbi_output))\n",
        "print(\"Violations for training data after viterbi is \", violations(viterbi_train_output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Violations for validation data after viterbi is  0\n",
            "Violations for training data after viterbi is  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swNZVZjemMg2",
        "colab_type": "text"
      },
      "source": [
        "**0 violations with Viterbi algorithm on top of CRF scores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCey4H26ZWYU",
        "colab_type": "code",
        "outputId": "ec989128-8e7a-454d-cea8-53117cbb5842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(\"Evaluation for validation data: \", span_stats(viterbi_output))\n",
        "print(\"Evaluation for training data: \",span_stats(viterbi_train_output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of True Positives/matches in both predicted output and gold standard: 9\n",
            "Number of NER spans in predicted output: 80\n",
            "Number of NER spans in gold standard: 43\n",
            "Evaluation for validation data:  {'precision': 0.1125, 'recall': 0.20930232558139536, 'f1': 0.14634146341463414}\n",
            "Number of True Positives/matches in both predicted output and gold standard: 478\n",
            "Number of NER spans in predicted output: 1182\n",
            "Number of NER spans in gold standard: 808\n",
            "Evaluation for training data:  {'precision': 0.40439932318104904, 'recall': 0.5915841584158416, 'f1': 0.4804020100502512}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlN-jJ4iNPpB",
        "colab_type": "text"
      },
      "source": [
        "The nuber of spans in predicted output increased compared to before viterbi, and hence reduction in true positive and F1 score."
      ]
    }
  ]
}